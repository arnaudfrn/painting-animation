{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"AL_main.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"XvuooXIJR2IH","colab_type":"code","outputId":"84e44bb4-d303-4d1f-ebbc-e77bb49ce421","executionInfo":{"status":"ok","timestamp":1572640546423,"user_tz":-60,"elapsed":25204,"user":{"displayName":"Benoit AUDIGIER","photoUrl":"","userId":"09297201092147045147"}},"colab":{"base_uri":"https://localhost:8080/","height":146}},"source":["# https://www.profillic.com/paper/arxiv:1910.07192?fbclid=IwAR2VT-TTa3L8dQ2thrkDVLrWljLe1Ng78PuNc3oezotj8vl-K5YZ32DARbo\n","\n","from google.colab import drive\n","drive.mount('/gdrive')\n","%cd '/gdrive/My Drive/NoArtist./Tech/Code/randomtest/Animating-Landscape'\n","# !git clone https://github.com/endo-yuki-t/Animating-Landscape.git\n","# models were found here \n","# http://www.cgg.cs.tsukuba.ac.jp/~endo/projects/AnimatingLandscape/ and added by hand in models/ (not all of them cause connection sucks dicks over here)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n","\n","Enter your authorization code:\n","··········\n","Mounted at /gdrive\n","/gdrive/My Drive/NoArtist./Tech/Code/randomtest/Animating-Landscape\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"kFWcX_xIRyzD","colab_type":"code","outputId":"c3925f9e-f5d2-4465-ee8e-20d56428759f","executionInfo":{"status":"ok","timestamp":1572641808067,"user_tz":-60,"elapsed":1218,"user":{"displayName":"Benoit AUDIGIER","photoUrl":"","userId":"09297201092147045147"}},"colab":{"base_uri":"https://localhost:8080/","height":235}},"source":["for i in range(4,16):\n","    print(f\"!python test.py --gpu 0 -i ./inputs_froquet/tmp{i}.jpg -o ./outputs_froquet\")"],"execution_count":0,"outputs":[{"output_type":"stream","text":["!python test.py --gpu 0 -i ./inputs_froquet/tmp4.jpg -o ./outputs_froquet\n","!python test.py --gpu 0 -i ./inputs_froquet/tmp5.jpg -o ./outputs_froquet\n","!python test.py --gpu 0 -i ./inputs_froquet/tmp6.jpg -o ./outputs_froquet\n","!python test.py --gpu 0 -i ./inputs_froquet/tmp7.jpg -o ./outputs_froquet\n","!python test.py --gpu 0 -i ./inputs_froquet/tmp8.jpg -o ./outputs_froquet\n","!python test.py --gpu 0 -i ./inputs_froquet/tmp9.jpg -o ./outputs_froquet\n","!python test.py --gpu 0 -i ./inputs_froquet/tmp10.jpg -o ./outputs_froquet\n","!python test.py --gpu 0 -i ./inputs_froquet/tmp11.jpg -o ./outputs_froquet\n","!python test.py --gpu 0 -i ./inputs_froquet/tmp12.jpg -o ./outputs_froquet\n","!python test.py --gpu 0 -i ./inputs_froquet/tmp13.jpg -o ./outputs_froquet\n","!python test.py --gpu 0 -i ./inputs_froquet/tmp14.jpg -o ./outputs_froquet\n","!python test.py --gpu 0 -i ./inputs_froquet/tmp15.jpg -o ./outputs_froquet\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"vj-xxFoTwdzx","colab_type":"code","outputId":"791d9906-af7e-456a-fd61-6ec6371dc3e6","executionInfo":{"status":"ok","timestamp":1572643643119,"user_tz":-60,"elapsed":1731518,"user":{"displayName":"Benoit AUDIGIER","photoUrl":"","userId":"09297201092147045147"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["!python test.py --gpu 0 -i ./inputs_froquet/tmp4.jpg -o ./outputs_froquet\n","!python test.py --gpu 0 -i ./inputs_froquet/tmp5.jpg -o ./outputs_froquet\n","!python test.py --gpu 0 -i ./inputs_froquet/tmp6.jpg -o ./outputs_froquet\n","!python test.py --gpu 0 -i ./inputs_froquet/tmp7.jpg -o ./outputs_froquet\n","!python test.py --gpu 0 -i ./inputs_froquet/tmp8.jpg -o ./outputs_froquet\n","!python test.py --gpu 0 -i ./inputs_froquet/tmp9.jpg -o ./outputs_froquet\n","!python test.py --gpu 0 -i ./inputs_froquet/tmp10.jpg -o ./outputs_froquet\n","!python test.py --gpu 0 -i ./inputs_froquet/tmp11.jpg -o ./outputs_froquet\n","!python test.py --gpu 0 -i ./inputs_froquet/tmp12.jpg -o ./outputs_froquet\n","!python test.py --gpu 0 -i ./inputs_froquet/tmp13.jpg -o ./outputs_froquet\n","!python test.py --gpu 0 -i ./inputs_froquet/tmp14.jpg -o ./outputs_froquet\n","!python test.py --gpu 0 -i ./inputs_froquet/tmp15.jpg -o ./outputs_froquet"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Motion: \n","Processing frame 1, /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1340: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n","/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2404: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\n","  warnings.warn(\"nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\")\n","/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2693: UserWarning: Default grid_sample and affine_grid behavior will be changed to align_corners=False from 1.4.0. See the documentation of grid_sample for details.\n","  warnings.warn(\"Default grid_sample and affine_grid behavior will be changed \"\n","Processing frame 199, VIDEOIO ERROR: V4L: can't open camera by index 0\n","VIDEOIO ERROR: V4L: can't open camera by index 0\n","\n","Appearance: \n","Processing frame 1010, VIDEOIO ERROR: V4L: can't open camera by index 0\n","\n","Done.\n","Motion: \n","Processing frame 1, /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1340: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n","/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2404: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\n","  warnings.warn(\"nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\")\n","/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2693: UserWarning: Default grid_sample and affine_grid behavior will be changed to align_corners=False from 1.4.0. See the documentation of grid_sample for details.\n","  warnings.warn(\"Default grid_sample and affine_grid behavior will be changed \"\n","Processing frame 199, tcmalloc: large alloc 6956883968 bytes == 0x7fd8f1566000 @  0x7fdb1cfd1001 0x7fdb1aaaf0f5 0x7fdb1ab124d9 0x7fdb1ab146af 0x7fdb1abaa9b8 0x50abc5 0x50c549 0x5081d5 0x50a020 0x50aa1d 0x50c549 0x509ce8 0x50aa1d 0x50c549 0x509ce8 0x50aa1d 0x50c549 0x5081d5 0x50b3a3 0x635082 0x635137 0x6388ef 0x639491 0x4b0f60 0x7fdb1cbccb97 0x5b2eaa\n","^C\n","Motion: \n","Processing frame 1, /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1340: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n","/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2404: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\n","  warnings.warn(\"nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\")\n","/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2693: UserWarning: Default grid_sample and affine_grid behavior will be changed to align_corners=False from 1.4.0. See the documentation of grid_sample for details.\n","  warnings.warn(\"Default grid_sample and affine_grid behavior will be changed \"\n","Processing frame 199, VIDEOIO ERROR: V4L: can't open camera by index 0\n","VIDEOIO ERROR: V4L: can't open camera by index 0\n","\n","Appearance: \n","Processing frame 1010, VIDEOIO ERROR: V4L: can't open camera by index 0\n","\n","Done.\n","Motion: \n","Processing frame 1, /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1340: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n","/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2404: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\n","  warnings.warn(\"nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\")\n","/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2693: UserWarning: Default grid_sample and affine_grid behavior will be changed to align_corners=False from 1.4.0. See the documentation of grid_sample for details.\n","  warnings.warn(\"Default grid_sample and affine_grid behavior will be changed \"\n","Processing frame 199, tcmalloc: large alloc 6043033600 bytes == 0x7f87c5eca000 @  0x7f89b4344001 0x7f89b1e220f5 0x7f89b1e854d9 0x7f89b1e876af 0x7f89b1f1d9b8 0x50abc5 0x50c549 0x5081d5 0x50a020 0x50aa1d 0x50c549 0x509ce8 0x50aa1d 0x50c549 0x509ce8 0x50aa1d 0x50c549 0x5081d5 0x50b3a3 0x635082 0x635137 0x6388ef 0x639491 0x4b0f60 0x7f89b3f3fb97 0x5b2eaa\n","^C\n","Motion: \n","Processing frame 1, /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1340: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n","/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2404: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\n","  warnings.warn(\"nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\")\n","/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2693: UserWarning: Default grid_sample and affine_grid behavior will be changed to align_corners=False from 1.4.0. See the documentation of grid_sample for details.\n","  warnings.warn(\"Default grid_sample and affine_grid behavior will be changed \"\n","Processing frame 199, tcmalloc: large alloc 1925259264 bytes == 0xb1aa4000 @  0x7f4a6491b001 0x7f4a623f90f5 0x7f4a6245c4d9 0x7f4a6245e6af 0x7f4a624f49b8 0x50abc5 0x50c549 0x5081d5 0x50a020 0x50aa1d 0x50c549 0x509ce8 0x50aa1d 0x50c549 0x509ce8 0x50aa1d 0x50c549 0x5081d5 0x50b3a3 0x635082 0x635137 0x6388ef 0x639491 0x4b0f60 0x7f4a64516b97 0x5b2eaa\n","VIDEOIO ERROR: V4L: can't open camera by index 0\n","VIDEOIO ERROR: V4L: can't open camera by index 0\n","\n","Appearance: \n","Processing frame 1010, VIDEOIO ERROR: V4L: can't open camera by index 0\n","\n","Done.\n","Motion: \n","Processing frame 1, /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1340: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n","/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2404: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\n","  warnings.warn(\"nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\")\n","/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2693: UserWarning: Default grid_sample and affine_grid behavior will be changed to align_corners=False from 1.4.0. See the documentation of grid_sample for details.\n","  warnings.warn(\"Default grid_sample and affine_grid behavior will be changed \"\n","Processing frame 199, tcmalloc: large alloc 1619238912 bytes == 0xa1492000 @  0x7f9c10188001 0x7f9c0dc660f5 0x7f9c0dcc94d9 0x7f9c0dccb6af 0x7f9c0dd619b8 0x50abc5 0x50c549 0x5081d5 0x50a020 0x50aa1d 0x50c549 0x509ce8 0x50aa1d 0x50c549 0x509ce8 0x50aa1d 0x50c549 0x5081d5 0x50b3a3 0x635082 0x635137 0x6388ef 0x639491 0x4b0f60 0x7f9c0fd83b97 0x5b2eaa\n","VIDEOIO ERROR: V4L: can't open camera by index 0\n","VIDEOIO ERROR: V4L: can't open camera by index 0\n","\n","Appearance: \n","Processing frame 1010, VIDEOIO ERROR: V4L: can't open camera by index 0\n","\n","Done.\n","Motion: \n","Processing frame 1, /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1340: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n","/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2404: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\n","  warnings.warn(\"nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\")\n","/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2693: UserWarning: Default grid_sample and affine_grid behavior will be changed to align_corners=False from 1.4.0. See the documentation of grid_sample for details.\n","  warnings.warn(\"Default grid_sample and affine_grid behavior will be changed \"\n","Processing frame 199, VIDEOIO ERROR: V4L: can't open camera by index 0\n","VIDEOIO ERROR: V4L: can't open camera by index 0\n","\n","Appearance: \n","Processing frame 1010, VIDEOIO ERROR: V4L: can't open camera by index 0\n","\n","Done.\n","Motion: \n","Processing frame 1, /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1340: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n","/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2404: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\n","  warnings.warn(\"nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\")\n","/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2693: UserWarning: Default grid_sample and affine_grid behavior will be changed to align_corners=False from 1.4.0. See the documentation of grid_sample for details.\n","  warnings.warn(\"Default grid_sample and affine_grid behavior will be changed \"\n","Processing frame 199, tcmalloc: large alloc 2301837312 bytes == 0xc42b4000 @  0x7fc0c8b56001 0x7fc0c66340f5 0x7fc0c66974d9 0x7fc0c66996af 0x7fc0c672f9b8 0x50abc5 0x50c549 0x5081d5 0x50a020 0x50aa1d 0x50c549 0x509ce8 0x50aa1d 0x50c549 0x509ce8 0x50aa1d 0x50c549 0x5081d5 0x50b3a3 0x635082 0x635137 0x6388ef 0x639491 0x4b0f60 0x7fc0c8751b97 0x5b2eaa\n","VIDEOIO ERROR: V4L: can't open camera by index 0\n","VIDEOIO ERROR: V4L: can't open camera by index 0\n","\n","Appearance: \n","Processing frame 1010, VIDEOIO ERROR: V4L: can't open camera by index 0\n","\n","Done.\n","Motion: \n","Processing frame 1, /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1340: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n","/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2404: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\n","  warnings.warn(\"nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\")\n","/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2693: UserWarning: Default grid_sample and affine_grid behavior will be changed to align_corners=False from 1.4.0. See the documentation of grid_sample for details.\n","  warnings.warn(\"Default grid_sample and affine_grid behavior will be changed \"\n","Processing frame 199, VIDEOIO ERROR: V4L: can't open camera by index 0\n","VIDEOIO ERROR: V4L: can't open camera by index 0\n","\n","Appearance: \n","Processing frame 1010, VIDEOIO ERROR: V4L: can't open camera by index 0\n","\n","Done.\n","Motion: \n","Processing frame 1, /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1340: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n","/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2404: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\n","  warnings.warn(\"nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\")\n","/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2693: UserWarning: Default grid_sample and affine_grid behavior will be changed to align_corners=False from 1.4.0. See the documentation of grid_sample for details.\n","  warnings.warn(\"Default grid_sample and affine_grid behavior will be changed \"\n","Processing frame 199, tcmalloc: large alloc 2424004608 bytes == 0xcb812000 @  0x7fdd3af56001 0x7fdd38a340f5 0x7fdd38a974d9 0x7fdd38a996af 0x7fdd38b2f9b8 0x50abc5 0x50c549 0x5081d5 0x50a020 0x50aa1d 0x50c549 0x509ce8 0x50aa1d 0x50c549 0x509ce8 0x50aa1d 0x50c549 0x5081d5 0x50b3a3 0x635082 0x635137 0x6388ef 0x639491 0x4b0f60 0x7fdd3ab51b97 0x5b2eaa\n","VIDEOIO ERROR: V4L: can't open camera by index 0\n","VIDEOIO ERROR: V4L: can't open camera by index 0\n","\n","Appearance: \n","Processing frame 1010, VIDEOIO ERROR: V4L: can't open camera by index 0\n","\n","Done.\n","Motion: \n","Processing frame 1, /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1340: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n","/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2404: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\n","  warnings.warn(\"nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\")\n","/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2693: UserWarning: Default grid_sample and affine_grid behavior will be changed to align_corners=False from 1.4.0. See the documentation of grid_sample for details.\n","  warnings.warn(\"Default grid_sample and affine_grid behavior will be changed \"\n","Processing frame 199, tcmalloc: large alloc 1141186560 bytes == 0x88bae000 @  0x7f7034464001 0x7f7031f420f5 0x7f7031fa54d9 0x7f7031fa76af 0x7f703203d9b8 0x50abc5 0x50c549 0x5081d5 0x50a020 0x50aa1d 0x50c549 0x509ce8 0x50aa1d 0x50c549 0x509ce8 0x50aa1d 0x50c549 0x5081d5 0x50b3a3 0x635082 0x635137 0x6388ef 0x639491 0x4b0f60 0x7f703405fb97 0x5b2eaa\n","VIDEOIO ERROR: V4L: can't open camera by index 0\n","VIDEOIO ERROR: V4L: can't open camera by index 0\n","\n","Appearance: \n","Processing frame 1010, VIDEOIO ERROR: V4L: can't open camera by index 0\n","\n","Done.\n","Motion: \n","Traceback (most recent call last):\n","  File \"test.py\", line 248, in <module>\n","    AS.GenerateVideo()\n","  File \"test.py\", line 225, in GenerateVideo\n","    V_mloop, V_f = self.PredictMotion()      \n","  File \"test.py\", line 61, in PredictMotion\n","    test_img = cv2.resize(test_img, (self.w,self.h))\n","cv2.error: OpenCV(3.4.3) /io/opencv/modules/imgproc/src/resize.cpp:4044: error: (-215:Assertion failed) !ssize.empty() in function 'resize'\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ae1daq9Oc4Lc","colab_type":"code","outputId":"05aa312b-43ae-415a-fa1a-b18f931340a0","executionInfo":{"status":"ok","timestamp":1572620266349,"user_tz":-60,"elapsed":56046,"user":{"displayName":"Benoit AUDIGIER","photoUrl":"","userId":"09297201092147045147"}},"colab":{"base_uri":"https://localhost:8080/","height":201}},"source":["!python test.py --gpu 0 -i ./inputs_froquet/tmp_png.png -o ./outputs_froquets"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Motion: \n","Processing frame 1, /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1340: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n","/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2404: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\n","  warnings.warn(\"nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\")\n","/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2693: UserWarning: Default grid_sample and affine_grid behavior will be changed to align_corners=False from 1.4.0. See the documentation of grid_sample for details.\n","  warnings.warn(\"Default grid_sample and affine_grid behavior will be changed \"\n","Processing frame 199, tcmalloc: large alloc 9696002048 bytes == 0x7ff500eac000 @  0x7ff80ae77001 0x7ff8089550f5 0x7ff8089b84d9 0x7ff8089ba6af 0x7ff808a509b8 0x50abc5 0x50c549 0x5081d5 0x50a020 0x50aa1d 0x50c549 0x509ce8 0x50aa1d 0x50c549 0x509ce8 0x50aa1d 0x50c549 0x5081d5 0x50b3a3 0x635082 0x635137 0x6388ef 0x639491 0x4b0f60 0x7ff80aa72b97 0x5b2eaa\n","^C\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"1Nbw96WDeFVe","colab_type":"code","outputId":"3956c718-5a55-40dd-e96b-0d30a6f2b74b","executionInfo":{"status":"ok","timestamp":1572620460433,"user_tz":-60,"elapsed":192702,"user":{"displayName":"Benoit AUDIGIER","photoUrl":"","userId":"09297201092147045147"}},"colab":{"base_uri":"https://localhost:8080/","height":310}},"source":["!python test.py --gpu 0 -i ./inputs_froquet/tmp2.jpg -o ./outputs_froquets"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Motion: \n","Processing frame 1, /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1340: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n","/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2404: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\n","  warnings.warn(\"nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\")\n","/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2693: UserWarning: Default grid_sample and affine_grid behavior will be changed to align_corners=False from 1.4.0. See the documentation of grid_sample for details.\n","  warnings.warn(\"Default grid_sample and affine_grid behavior will be changed \"\n","Processing frame 199, tcmalloc: large alloc 1897996288 bytes == 0xaf7d6000 @  0x7f8dd6eee001 0x7f8dd49cc0f5 0x7f8dd4a2f4d9 0x7f8dd4a316af 0x7f8dd4ac79b8 0x50abc5 0x50c549 0x5081d5 0x50a020 0x50aa1d 0x50c549 0x509ce8 0x50aa1d 0x50c549 0x509ce8 0x50aa1d 0x50c549 0x5081d5 0x50b3a3 0x635082 0x635137 0x6388ef 0x639491 0x4b0f60 0x7f8dd6ae9b97 0x5b2eaa\n","VIDEOIO ERROR: V4L: can't open camera by index 0\n","VIDEOIO ERROR: V4L: can't open camera by index 0\n","\n","Appearance: \n","Processing frame 1010, VIDEOIO ERROR: V4L: can't open camera by index 0\n","\n","Done.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"EJkodZEZeFY2","colab_type":"code","outputId":"7e04ced6-7c34-4633-d114-26cbc9a8c6e6","executionInfo":{"status":"ok","timestamp":1572620466687,"user_tz":-60,"elapsed":198951,"user":{"displayName":"Benoit AUDIGIER","photoUrl":"","userId":"09297201092147045147"}},"colab":{"base_uri":"https://localhost:8080/","height":199}},"source":["!python test.py --gpu 0 -i ./inputs_froquet/tmp3.jpg -o ./outputs_froquet"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Motion: \n","Traceback (most recent call last):\n","  File \"test.py\", line 248, in <module>\n","    AS.GenerateVideo()\n","  File \"test.py\", line 225, in GenerateVideo\n","    V_mloop, V_f = self.PredictMotion()      \n","  File \"test.py\", line 61, in PredictMotion\n","    test_img = cv2.resize(test_img, (self.w,self.h))\n","cv2.error: OpenCV(3.4.3) /io/opencv/modules/imgproc/src/resize.cpp:4044: error: (-215:Assertion failed) !ssize.empty() in function 'resize'\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"6w28GLOPekc8","colab_type":"code","outputId":"d2ff77de-85e0-4d8d-b078-5ce066ea78fc","executionInfo":{"status":"ok","timestamp":1572620521644,"user_tz":-60,"elapsed":53860,"user":{"displayName":"Benoit AUDIGIER","photoUrl":"","userId":"09297201092147045147"}},"colab":{"base_uri":"https://localhost:8080/","height":201}},"source":["!python test.py --gpu 0 -i ./inputs_froquet/tmp_png.png -o ./outputs_froquet"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Motion: \n","Processing frame 1, /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1340: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n","/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2404: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\n","  warnings.warn(\"nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\")\n","/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2693: UserWarning: Default grid_sample and affine_grid behavior will be changed to align_corners=False from 1.4.0. See the documentation of grid_sample for details.\n","  warnings.warn(\"Default grid_sample and affine_grid behavior will be changed \"\n","Processing frame 199, tcmalloc: large alloc 9696002048 bytes == 0x7f964db2c000 @  0x7f9957d1e001 0x7f99557fc0f5 0x7f995585f4d9 0x7f99558616af 0x7f99558f79b8 0x50abc5 0x50c549 0x5081d5 0x50a020 0x50aa1d 0x50c549 0x509ce8 0x50aa1d 0x50c549 0x509ce8 0x50aa1d 0x50c549 0x5081d5 0x50b3a3 0x635082 0x635137 0x6388ef 0x639491 0x4b0f60 0x7f9957919b97 0x5b2eaa\n","^C\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"AwNiJHjGen9M","colab_type":"code","outputId":"867ba091-1c3b-466e-b9ab-1054faf827d1","executionInfo":{"status":"ok","timestamp":1572620721191,"user_tz":-60,"elapsed":253386,"user":{"displayName":"Benoit AUDIGIER","photoUrl":"","userId":"09297201092147045147"}},"colab":{"base_uri":"https://localhost:8080/","height":310}},"source":["!python test.py --gpu 0 -i ./inputs_froquet/tmp2.jpg -o ./outputs_froquet"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Motion: \n","Processing frame 1, /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1340: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n","/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2404: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\n","  warnings.warn(\"nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\")\n","/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2693: UserWarning: Default grid_sample and affine_grid behavior will be changed to align_corners=False from 1.4.0. See the documentation of grid_sample for details.\n","  warnings.warn(\"Default grid_sample and affine_grid behavior will be changed \"\n","Processing frame 199, tcmalloc: large alloc 1897996288 bytes == 0xaebaa000 @  0x7f94018dd001 0x7f93ff3bb0f5 0x7f93ff41e4d9 0x7f93ff4206af 0x7f93ff4b69b8 0x50abc5 0x50c549 0x5081d5 0x50a020 0x50aa1d 0x50c549 0x509ce8 0x50aa1d 0x50c549 0x509ce8 0x50aa1d 0x50c549 0x5081d5 0x50b3a3 0x635082 0x635137 0x6388ef 0x639491 0x4b0f60 0x7f94014d8b97 0x5b2eaa\n","VIDEOIO ERROR: V4L: can't open camera by index 0\n","VIDEOIO ERROR: V4L: can't open camera by index 0\n","\n","Appearance: \n","Processing frame 1010, VIDEOIO ERROR: V4L: can't open camera by index 0\n","\n","Done.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Dc0eae7AekhY","colab_type":"code","outputId":"390ee0cc-495d-40d9-988f-f72072862f03","executionInfo":{"status":"ok","timestamp":1572620895672,"user_tz":-60,"elapsed":166756,"user":{"displayName":"Benoit AUDIGIER","photoUrl":"","userId":"09297201092147045147"}},"colab":{"base_uri":"https://localhost:8080/","height":291}},"source":["!python test.py --gpu 0 -i ./inputs_froquet/tmp3.png -o ./outputs_froquet"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Motion: \n","Processing frame 1, /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1340: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n","/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2404: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\n","  warnings.warn(\"nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\")\n","/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2693: UserWarning: Default grid_sample and affine_grid behavior will be changed to align_corners=False from 1.4.0. See the documentation of grid_sample for details.\n","  warnings.warn(\"Default grid_sample and affine_grid behavior will be changed \"\n","Processing frame 199, VIDEOIO ERROR: V4L: can't open camera by index 0\n","VIDEOIO ERROR: V4L: can't open camera by index 0\n","\n","Appearance: \n","Processing frame 1010, VIDEOIO ERROR: V4L: can't open camera by index 0\n","\n","Done.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"kDoEZZzyekku","colab_type":"code","outputId":"4fd621d7-ba27-41ac-fa1f-e7a92e0e77c9","executionInfo":{"status":"ok","timestamp":1572620947409,"user_tz":-60,"elapsed":213900,"user":{"displayName":"Benoit AUDIGIER","photoUrl":"","userId":"09297201092147045147"}},"colab":{"base_uri":"https://localhost:8080/","height":201}},"source":["!python test.py --gpu 0 -i ./inputs_froquet/tmp.jpg -o ./outputs_froquet"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Motion: \n","Processing frame 1, /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1340: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n","/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2404: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\n","  warnings.warn(\"nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\")\n","/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2693: UserWarning: Default grid_sample and affine_grid behavior will be changed to align_corners=False from 1.4.0. See the documentation of grid_sample for details.\n","  warnings.warn(\"Default grid_sample and affine_grid behavior will be changed \"\n","Processing frame 199, tcmalloc: large alloc 9696002048 bytes == 0x7f010db2c000 @  0x7f0417aac001 0x7f041558a0f5 0x7f04155ed4d9 0x7f04155ef6af 0x7f04156859b8 0x50abc5 0x50c549 0x5081d5 0x50a020 0x50aa1d 0x50c549 0x509ce8 0x50aa1d 0x50c549 0x509ce8 0x50aa1d 0x50c549 0x5081d5 0x50b3a3 0x635082 0x635137 0x6388ef 0x639491 0x4b0f60 0x7f04176a7b97 0x5b2eaa\n","^C\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"2SbxCzdoVzd8","colab_type":"code","outputId":"a8b66c9c-a50f-4d82-9c0c-baf3978812c0","executionInfo":{"status":"error","timestamp":1572618083986,"user_tz":-60,"elapsed":2508,"user":{"displayName":"Benoit AUDIGIER","photoUrl":"","userId":"09297201092147045147"}},"colab":{"base_uri":"https://localhost:8080/","height":396}},"source":["# CODE OF TEST\n","\n","# encoding: utf-8\n","\n","from __future__ import print_function\n","import argparse, os, pickle, sys\n","import numpy as np\n","import cv2\n","import torch\n","import torch.nn.functional as F\n","from torch.autograd import Variable\n","from predictor import ConditionalMotionNet, ConditionalAppearanceNet\n","from encoder import define_E\n","from util import generateLoop, videoWrite, normalize, denormalize\n","\n","class AnimatingLandscape():\n","    \n","    def __init__(self, args):\n","        self.model_path = args.model_path\n","        self.model_epoch = args.model_epoch\n","        self.gpu = int(args.gpu)\n","        self.input_path = args.input\n","        self.outdir_path = args.outdir\n","        self.t_m = float(args.motion_latent_code)\n","        self.s_m = float(args.motion_speed)\n","        self.t_a = float(args.appearance_latent_code)\n","        self.s_a = float(args.appearance_speed)\n","        self.t_m = min(1.,max(0.,self.t_m))\n","        self.s_m = min(1.,max(1e-3,self.s_m))\n","        self.t_a = min(1.,max(0.,self.t_a))\n","        self.s_a = min(1.,max(1e-3,self.s_a))\n","        self.TM = int(args.motion_frame_number)\n","        self.w, self.h = 256, 256 #Image size for network input\n","        self.fw, self.fh = None, None #Output image size\n","        self.pad = 64 #Reflection padding size for sampling outside of the image\n","    \n","    def PredictMotion(self):  \n","        print('Motion: ')\n","        P_m = ConditionalMotionNet()\n","        param = torch.load(self.model_path + '/PMNet_weight_' + self.model_epoch + '.pth')\n","        P_m.load_state_dict(param)\n","        if self.gpu>-1:\n","            P_m.cuda(self.gpu)\n","        \n","        with open(self.model_path + '/codebook_m_' + self.model_epoch + '.pkl', 'rb') as f:\n","            codebook_m = pickle.load(f) if sys.version_info[0] == 2 else pickle.load(f, encoding='latin1')\n","    \n","        id1 = int(np.floor((len(codebook_m)-1)*self.t_m))\n","        id2 = int(np.ceil((len(codebook_m)-1)*self.t_m))\n","        z_weight = (len(codebook_m)-1)*self.t_m-np.floor((len(codebook_m)-1)*self.t_m)\n","        z_m = (1.-z_weight)*codebook_m[id1:id1+1]+z_weight*codebook_m[id2:id2+1]\n","        z_m = Variable(torch.from_numpy(z_m.astype(np.float32)))\n","        if self.gpu>-1:\n","            z_m = z_m.cuda(self.gpu)\n","        initial_coordinate = np.array([np.meshgrid(np.linspace(-1,1,self.w+2*self.pad), np.linspace(-1,1,self.h+2*self.pad), sparse=False)]).astype(np.float32)\n","        initial_coordinate = Variable(torch.from_numpy(initial_coordinate))\n","        if self.gpu>-1:\n","            initial_coordinate = initial_coordinate.cuda(self.gpu)\n","        \n","        with torch.no_grad():\n","            \n","            test_img = cv2.imread(self.input_path)\n","            test_img = cv2.resize(test_img, (self.w,self.h))\n","            test_input = np.array([normalize(test_img)])\n","            test_input = Variable(torch.from_numpy(test_input.transpose(0,3,1,2)))       \n","            if self.gpu>-1:\n","                test_input = test_input.cuda(self.gpu)\n","            padded_test_input = F.pad(test_input, (self.pad,self.pad,self.pad,self.pad), mode='reflect')\n","            \n","            test_img_large = cv2.imread(self.input_path)\n","            if self.fw == None or self.fh == None:\n","                self.fh, self.fw = test_img_large.shape[:2]\n","            test_img_large = cv2.resize(test_img_large, (self.fw, self.fh))\n","            padded_test_input_large = np.array([normalize(test_img_large)])\n","            padded_test_input_large = Variable(torch.from_numpy(padded_test_input_large.transpose(0,3,1,2)))\n","            if self.gpu>-1:\n","                padded_test_input_large = padded_test_input_large.cuda(self.gpu)\n","            scaled_pads = (int(self.pad*self.fh/float(self.h)), int(self.pad*self.fw/float(self.w)))\n","            padded_test_input_large = F.pad(padded_test_input_large, (scaled_pads[1],scaled_pads[1],scaled_pads[0],scaled_pads[0]), mode='reflect')\n","            \n","            V_m = list()\n","            V_f = list()\n","            old_correpondence = None\n","            for t in range(self.TM):\n","                sys.stdout.write(\"\\rProcessing frame %d, \" % (t+1))\n","                sys.stdout.flush()\n","                \n","                flow = P_m(test_input, z_m)\n","                flow[:,0,:,:] = flow[:,0,:,:]*(self.w/float(self.pad*2+self.w))\n","                flow[:,1,:,:] = flow[:,1,:,:]*(self.h/float(self.pad*2+self.h))\n","                flow = F.pad(flow, (self.pad,self.pad,self.pad,self.pad), mode='reflect')\n","                flow = self.s_m*flow\n","                correspondence = initial_coordinate + flow\n","    \n","                if old_correpondence is not None:\n","                    correspondence = F.grid_sample(old_correpondence, correspondence.permute(0,2,3,1), padding_mode='border')\n","    \n","                correspondence_large = F.upsample(correspondence, size=(self.fh+scaled_pads[0]*2,self.fw+scaled_pads[1]*2), mode='bilinear', align_corners=True)\n","                y_large = F.grid_sample(padded_test_input_large, correspondence_large.permute(0,2,3,1), padding_mode='border')\n","                outimg = y_large.data.cpu().numpy()[0].transpose(1,2,0)\n","                outimg = denormalize(outimg)\n","                outimg = outimg[scaled_pads[0]:outimg.shape[0]-scaled_pads[0],scaled_pads[1]:outimg.shape[1]-scaled_pads[1]]\n","                V_m.append(outimg)\n","                \n","                outflowimg = flow.data.cpu().numpy()[0].transpose(1,2,0)\n","                outflowimg = outflowimg[self.pad:outflowimg.shape[0]-self.pad,self.pad:outflowimg.shape[1]-self.pad]\n","                mag, ang = cv2.cartToPolar(outflowimg[...,1], outflowimg[...,0])\n","                hsv = np.zeros_like(test_img)\n","                hsv[...,1] = cv2.normalize(mag,None,0,255,cv2.NORM_MINMAX)\n","                hsv[...,0] = ang*180/np.pi/2\n","                hsv[...,2] = 255\n","                outflowimg = cv2.cvtColor(hsv,cv2.COLOR_HSV2BGR)\n","                outflowimg = cv2.resize(outflowimg,(self.fw,self.fh))\n","                V_f.append(outflowimg)\n","                \n","                y = F.grid_sample(padded_test_input, correspondence.permute(0,2,3,1), padding_mode='border')\n","                test_input = y[:,:,self.pad:y.shape[2]-self.pad,self.pad:y.shape[3]-self.pad]\n","                old_correpondence = correspondence\n","                \n","            V_mloop = generateLoop(V_m)\n","            \n","        return V_mloop, V_f\n","    \n","    def PredictAppearance(self, V_mloop):\n","        print('\\nAppearance: ',)\n","        minimum_loop_num = int(1/self.s_a)\n","        P_a = ConditionalAppearanceNet(8)\n","        param = torch.load(self.model_path + '/PANet_weight_' + self.model_epoch + '.pth')\n","        P_a.load_state_dict(param)\n","        if self.gpu>-1:\n","            P_a.cuda(self.gpu)\n","        E_a = define_E(3,8,64,which_model_netE='resnet_128',vaeLike=True)\n","        param = torch.load(self.model_path + '/EANet_weight_' + self.model_epoch + '.pth')\n","        E_a.load_state_dict(param)\n","        if self.gpu>-1:\n","            E_a.cuda(self.gpu)\n","    \n","        with torch.no_grad():  \n","            interpolated_za_seq = list()         \n","            \n","            input_conditional_test = cv2.resize(V_mloop[0], (128, 128))\n","            input_conditional_test = np.array([normalize(input_conditional_test)])\n","            input_conditional_test = Variable(torch.from_numpy(input_conditional_test.transpose(0,3,1,2)))\n","            if self.gpu>-1:\n","                input_conditional_test = input_conditional_test.cuda(self.gpu)\n","            za_input, _ = E_a(input_conditional_test)\n","            interpolated_za_seq.append(za_input.clone())\n","    \n","            with open(self.model_path + '/codebook_a_' + self.model_epoch + '.pkl', 'rb') as f:\n","                codebook_a = pickle.load(f) if sys.version_info[0] == 2 else pickle.load(f, encoding='latin1')\n","            za_seq = codebook_a[int((len(codebook_a)-1)*self.t_a)]\n","            za_seq = [torch.from_numpy(np.array([za])) for za in za_seq]\n","            if self.gpu>-1:\n","                za_seq = [za.cuda(self.gpu) for za in za_seq]\n","            start_fid = None\n","            min_dist = float('inf')\n","            for t, mu in enumerate(za_seq):\n","                dist = F.mse_loss(za_input,mu).cpu().numpy()\n","                if dist < min_dist:\n","                    min_dist = dist\n","                    start_fid = t\n","    \n","            TA = len(za_seq)\n","            loop_num = max(minimum_loop_num, int(np.ceil(len(za_seq)/float(len(V_mloop)))))\n","            interpolation_size = int((loop_num*len(V_mloop)-TA)/TA)\n","            za1 = za_input.clone()\n","            for t in range(start_fid+1,TA):\n","                za2 = za_seq[t]\n","                for ti in range(interpolation_size):\n","                    lambd = (ti+1)/float(interpolation_size+1)\n","                    z = (1.-lambd)*za1 + lambd*za2\n","                    interpolated_za_seq.append(z)\n","                interpolated_za_seq.append(za2)\n","                za1 = za2\n","        \n","            za1 = za_input.clone()\n","            for t in range(start_fid-1,-1,-1):\n","                za2 = za_seq[t]    \n","                for ti in range(interpolation_size-1,-1,-1):\n","                    lambd = (ti+1)/float(interpolation_size+1)\n","                    z = (1.-lambd)*za2 + lambd*za1\n","                    interpolated_za_seq.insert(0, z)\n","                interpolated_za_seq.insert(0,za2)\n","                za1 = za2\n","    \n","            loop_num = int(np.ceil(TA*(interpolation_size+1)/float(len(V_mloop))))\n","            interpolation_size2 = int(interpolation_size+loop_num*len(V_mloop)-TA*(interpolation_size+1))\n","            z_start = za_input.clone() if start_fid==0 else za_seq[0]\n","            z_final = za_input.clone() if start_fid==TA-1 else za_seq[-1]\n","            for ti in range(interpolation_size2):\n","                lambd = (ti+1)/float(interpolation_size2+1)\n","                z = (1.-lambd)*z_final + lambd*z_start\n","                interpolated_za_seq.append(z)\n","    \n","            zaid=(interpolation_size+1)*start_fid\n","            V = list()\n","            t = 0\n","            for loop in range(loop_num):\n","                for frame in V_mloop:\n","                    sys.stdout.write(\"\\rProcessing frame %d, \" % (t+1))\n","                    sys.stdout.flush()\n","                    t+=1\n","                    \n","                    test_input = cv2.resize(frame, (self.w, self.h))\n","                    test_input = np.array([normalize(test_input)])\n","                    test_input = Variable(torch.from_numpy(test_input.transpose(0,3,1,2)))\n","                    if self.gpu>-1:\n","                        test_input = test_input.cuda(self.gpu)\n","                    test_input_large = np.array([normalize(frame)])\n","                    test_input_large = Variable(torch.from_numpy(test_input_large.transpose(0,3,1,2)))\n","                    if self.gpu>-1:\n","                        test_input_large = test_input_large.cuda(self.gpu)\n","                    z = interpolated_za_seq[zaid]\n","                    y, al, bl = P_a(test_input, z)\n","                    al_large = F.upsample(al, size=(self.fh,self.fw), mode='bilinear', align_corners=True)\n","                    bl_large = F.upsample(bl, size=(self.fh,self.fw), mode='bilinear', align_corners=True)\n","                    y = F.tanh(al_large*test_input_large+bl_large)\n","                    outimg = y.data.cpu().numpy()[0].transpose(1,2,0)\n","                    V.append(denormalize(outimg))\n","                    zaid+=1\n","                    if zaid>len(interpolated_za_seq)-1:\n","                        zaid=0\n","        \n","        return V\n","        \n","    def GenerateVideo(self):\n","        V_mloop, V_f = self.PredictMotion()      \n","        videoWrite(V_mloop, out_path = self.outdir_path + '/' + os.path.splitext(self.input_path)[0].split('/')[-1] + '_motion.avi')\n","        videoWrite(V_f, out_path = self.outdir_path + '/' + os.path.splitext(self.input_path)[0].split('/')[-1] + '_flow.avi')\n","        \n","        V = self.PredictAppearance(V_mloop)                \n","        videoWrite(V, out_path = self.outdir_path + '/' + os.path.splitext(self.input_path)[0].split('/')[-1] + '.avi')\n","        print('\\nDone.')\n","\n","# if __name__ == '__main__':\n","parser = argparse.ArgumentParser(description='AnimatingLandscape')\n","parser.add_argument('--model_path', default='./models')\n","parser.add_argument('--model_epoch', default='5000')\n","parser.add_argument('--gpu', default=-1)\n","parser.add_argument('--input', '-i', default='./inputs/1.png')\n","parser.add_argument('--motion_latent_code', '-mz', default=np.random.rand())\n","parser.add_argument('--motion_speed', '-ms', default=0.2)\n","parser.add_argument('--appearance_latent_code', '-az', default=np.random.rand())\n","parser.add_argument('--appearance_speed', '-as', default=0.1)\n","parser.add_argument('--motion_frame_number', '-mn', default=199)\n","parser.add_argument('--outdir', '-o', default='./outputs')\n","args = parser.parse_args('--gpu=0 -i=./inputs/1.png -o=./outputs'.split())\n","\n","AS = AnimatingLandscape(args)\n","AS.GenerateVideo()\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Motion: \n"],"name":"stdout"},{"output_type":"error","ename":"RuntimeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-12-e30c9634a8df>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    245\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m \u001b[0mAS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAnimatingLandscape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 247\u001b[0;31m \u001b[0mAS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGenerateVideo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-12-e30c9634a8df>\u001b[0m in \u001b[0;36mGenerateVideo\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mGenerateVideo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m         \u001b[0mV_mloop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mV_f\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPredictMotion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m         \u001b[0mvideoWrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mV_mloop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutdir_path\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplitext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'_motion.avi'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m         \u001b[0mvideoWrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mV_f\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutdir_path\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplitext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'_flow.avi'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-12-e30c9634a8df>\u001b[0m in \u001b[0;36mPredictMotion\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Motion: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0mP_m\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mConditionalMotionNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m         \u001b[0mparam\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_path\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/PMNet_weight_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_epoch\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'.pth'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m         \u001b[0mP_m\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgpu\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    424\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion_info\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m'encoding'\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpickle_load_args\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m             \u001b[0mpickle_load_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'encoding'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 426\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    427\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnew_fd\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_load\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    611\u001b[0m     \u001b[0munpickler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUnpickler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    612\u001b[0m     \u001b[0munpickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpersistent_load\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpersistent_load\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 613\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munpickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    614\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    615\u001b[0m     \u001b[0mdeserialized_storage_keys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mpersistent_load\u001b[0;34m(saved_id)\u001b[0m\n\u001b[1;32m    574\u001b[0m                 \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    575\u001b[0m                 \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_torch_load_uninitialized\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 576\u001b[0;31m                 \u001b[0mdeserialized_objects\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mroot_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrestore_location\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    577\u001b[0m             \u001b[0mstorage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeserialized_objects\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mroot_key\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mview_metadata\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mdefault_restore_location\u001b[0;34m(storage, location)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdefault_restore_location\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_package_registry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 155\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    156\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_cuda_deserialize\u001b[0;34m(obj, location)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_cuda_deserialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cuda'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidate_cuda_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"_torch_load_uninitialized\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m             \u001b[0mstorage_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mvalidate_cuda_device\u001b[0;34m(location)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m         raise RuntimeError('Attempting to deserialize object on a CUDA '\n\u001b[0m\u001b[1;32m    116\u001b[0m                            \u001b[0;34m'device but torch.cuda.is_available() is False. '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m                            \u001b[0;34m'If you are running on a CPU-only machine, '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are running on a CPU-only machine, please use torch.load with map_location=torch.device('cpu') to map your storages to the CPU."]}]},{"cell_type":"code","metadata":{"id":"549u94rMWXRh","colab_type":"code","outputId":"10271bff-82a1-4497-caf0-f81bc5d70360","executionInfo":{"status":"ok","timestamp":1572618224812,"user_tz":-60,"elapsed":558,"user":{"displayName":"Benoit AUDIGIER","photoUrl":"","userId":"09297201092147045147"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["\n","accelerator = 'cu80' if os.path.exists('/opt/bin/nvidia-smi') else 'cpu'\n","\n","accelerator\n"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'cpu'"]},"metadata":{"tags":[]},"execution_count":15}]},{"cell_type":"code","metadata":{"id":"L8ijWIMoSJ1T","colab_type":"code","colab":{}},"source":["!python test.py --gpu 0 -i ./inputs/1.png -o ./outputs -mz 0.9 -az 0.1  "],"execution_count":0,"outputs":[]}]}